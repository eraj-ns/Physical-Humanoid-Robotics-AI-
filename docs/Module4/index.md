# Vision-Language-Action

Welcome to Module 4! In this module, we will explore the exciting world of Vision-Language-Action (VLA) models and how they are used to create intelligent, autonomous robots.

---

# Module 4 â€“ Vision-Language-Action (VLA)

## Focus  
**The convergence of LLMs and Robotics**

This module connects large language models with real robotic actions, enabling humanoid robots to understand voice commands and perform intelligent task planning.

## Chapters in This Module

- Voice-to-Action: Using OpenAI Whisper for Voice Commands  
- Cognitive Planning: Using LLMs to Translate Natural Language into ROS 2 Actions  
- Capstone Project: The Autonomous Humanoid  

### Capstone Project
A simulated humanoid robot will:
- Receive a voice command  
- Plan a navigation path  
- Avoid obstacles  
- Detect objects using computer vision  
- Manipulate the target object autonomously  

This is the final integration of all modules.

